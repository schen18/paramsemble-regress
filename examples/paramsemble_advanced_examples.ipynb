{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Paramsemble Advanced Examples",
        "",
        "This notebook demonstrates advanced usage patterns and real-world scenarios:",
        "- Working with real datasets",
        "- Hyperparameter tuning",
        "- Feature importance analysis",
        "- Cross-validation strategies",
        "- Production deployment workflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np",
        "import pandas as pd",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "from sklearn.datasets import fetch_california_housing, load_diabetes",
        "from sklearn.model_selection import train_test_split",
        "from sklearn.preprocessing import StandardScaler",
        "from paramsemble import ParamsembleRegressor",
        "import json",
        "",
        "np.random.seed(42)",
        "sns.set_style('whitegrid')",
        "plt.rcParams['figure.figsize'] = (14, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: California Housing Dataset\n",
        "\n",
        "Let's apply ELM to the California Housing dataset to predict median house values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.Series(housing.target, name='MedHouseValue')\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"\\nFeatures:\")\n",
        "print(X.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X),\n",
        "    columns=X.columns\n",
        ")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Paramsemble modelregressor_housing = ParamsembleRegressor(    m=100,    f=4,    sample='unique',    method='elastic',    spread=15,    modeljson='models/housing_model.json',    random_state=42)print(\"Training Paramsemble on California Housing data...\")regressor_housing.fit(X_train, y_train, X_test, y_test)print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate performance",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error",
        "",
        "y_pred = regressor_housing.predict(X_test)",
        "",
        "print(\"\\n=== Model Performance ===\")",
        "print(f\"Baseline R²: {regressor_housing.baseline_metrics_['r2']:.4f}\")",
        "print(f\"Ensemble R²: {r2_score(y_test, y_pred):.4f}\")",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Hyperparameter Tuning\n",
        "\n",
        "Compare different hyperparameter configurations to find the best setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different configurations",
        "configs = [",
        "    {'m': 50, 'f': 3, 'spread': 10, 'method': 'elastic'},",
        "    {'m': 100, 'f': 4, 'spread': 15, 'method': 'elastic'},",
        "    {'m': 150, 'f': 5, 'spread': 20, 'method': 'elastic'},",
        "    {'m': 100, 'f': 4, 'spread': 15, 'method': 'mars'},",
        "]",
        "",
        "results = []",
        "",
        "for i, config in enumerate(configs):",
        "    print(f\"\\nTesting configuration {i+1}/{len(configs)}: {config}\")",
        "    ",
        "    regressor = ParamsembleRegressor(",
        "        m=config['m'],",
        "        f=config['f'],",
        "        spread=config['spread'],",
        "        method=config['method'],",
        "        sample='unique',",
        "        random_state=42",
        "    )",
        "    ",
        "    paramsemble.fit(X_train, y_train, X_test, y_test)",
        "    y_pred = paramsemble.predict(X_test)",
        "    ",
        "    r2 = r2_score(y_test, y_pred)",
        "    mae = mean_absolute_error(y_test, y_pred)",
        "    ",
        "    results.append({",
        "        'config': f\"m={config['m']}, f={config['f']}, spread={config['spread']}, {config['method']}\",",
        "        'r2': r2,",
        "        'mae': mae",
        "    })",
        "    ",
        "    print(f\"  R²: {r2:.4f}, MAE: {mae:.4f}\")",
        "",
        "results_df = pd.DataFrame(results)",
        "print(\"\\n=== Configuration Comparison ===\")",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize hyperparameter comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# R² comparison\n",
        "axes[0].barh(range(len(results_df)), results_df['r2'], color='steelblue', alpha=0.7)\n",
        "axes[0].set_yticks(range(len(results_df)))\n",
        "axes[0].set_yticklabels([f\"Config {i+1}\" for i in range(len(results_df))])\n",
        "axes[0].set_xlabel('R² Score', fontsize=12)\n",
        "axes[0].set_title('R² Score by Configuration', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# MAE comparison\n",
        "axes[1].barh(range(len(results_df)), results_df['mae'], color='coral', alpha=0.7)\n",
        "axes[1].set_yticks(range(len(results_df)))\n",
        "axes[1].set_yticklabels([f\"Config {i+1}\" for i in range(len(results_df))])\n",
        "axes[1].set_xlabel('Mean Absolute Error', fontsize=12)\n",
        "axes[1].set_title('MAE by Configuration', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Feature Importance Analysis\n",
        "\n",
        "Analyze which features appear most frequently in the selected constituent models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count feature appearances in selected models",
        "feature_counts = {}",
        "",
        "for model in regressor_housing.selected_models_:",
        "    for feature in model['features']:",
        "        feature_counts[feature] = feature_counts.get(feature, 0) + 1",
        "",
        "# Create DataFrame and sort",
        "feature_importance = pd.DataFrame([",
        "    {'Feature': feature, 'Count': count, 'Percentage': count / len(regressor_housing.selected_models_) * 100}",
        "    for feature, count in feature_counts.items()",
        "]).sort_values('Count', ascending=False)",
        "",
        "print(\"\\n=== Feature Importance (by appearance in selected models) ===\")",
        "print(feature_importance.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize feature importance",
        "fig, ax = plt.subplots(figsize=(12, 6))",
        "",
        "bars = ax.barh(feature_importance['Feature'], feature_importance['Count'], ",
        "               color='teal', alpha=0.7)",
        "ax.set_xlabel('Number of Appearances in Selected Models', fontsize=12)",
        "ax.set_ylabel('Feature', fontsize=12)",
        "ax.set_title('Feature Importance in Paramsemble Ensemble', fontsize=14, fontweight='bold')",
        "ax.grid(True, alpha=0.3, axis='x')",
        "",
        "# Add percentage labels",
        "for i, (bar, pct) in enumerate(zip(bars, feature_importance['Percentage'])):",
        "    width = bar.get_width()",
        "    ax.text(width, bar.get_y() + bar.get_height()/2,",
        "            f' {pct:.1f}%',",
        "            ha='left', va='center', fontsize=10)",
        "",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Model Inspection\n",
        "\n",
        "Examine the saved model JSON to understand the ensemble structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and inspect model JSON\n",
        "with open('models/housing_model.json', 'r') as f:\n",
        "    model_data = json.load(f)\n",
        "\n",
        "print(\"=== Model Structure ===\")\n",
        "print(f\"Ensemble method: {model_data['method']}\")\n",
        "print(f\"Number of constituent models: {len(model_data['constituent_models'])}\")\n",
        "print(f\"\\nMetadata:\")\n",
        "for key, value in model_data['metadata'].items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine a constituent model\n",
        "print(\"\\n=== Example Constituent Model ===\")\n",
        "example_model = model_data['constituent_models'][0]\n",
        "print(f\"Model ID: {example_model['model_id']}\")\n",
        "print(f\"Features: {example_model['features']}\")\n",
        "print(f\"wMAPE: {example_model['wmape']:.4f}\")\n",
        "print(f\"R²: {example_model['r2']:.4f}\")\n",
        "print(f\"\\nEquation coefficients:\")\n",
        "for feature, coef in example_model['equation_dict'].items():\n",
        "    print(f\"  {feature}: {coef:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine ensemble equation\n",
        "print(\"\\n=== Ensemble Equation ===\")\n",
        "print(\"Coefficients for combining constituent model predictions:\")\n",
        "for key, value in model_data['ensemble_equation'].items():\n",
        "    print(f\"  {key}: {value:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5: Production Deployment Workflow",
        "",
        "Demonstrate a complete workflow for deploying Paramsemble models in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Train and save modelprint(\"Step 1: Training production model...\")regressor_prod = ParamsembleRegressor(    m=100,    f=4,    spread=15,    method='elastic',    modeljson='models/production_model.json',    random_state=42)regressor_prod.fit(X_train, y_train, X_test, y_test)print(\"✓ Model trained and saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Export to SQL for database deployment",
        "print(\"\\nStep 2: Exporting to SQL...\")",
        "sql_code = regressor_prod.export_sql(",
        "    'models/production_model.json',",
        "    table_name='housing_features',",
        "    id_column='property_id'",
        ")",
        "",
        "with open('models/production_model.sql', 'w') as f:",
        "    f.write(sql_code)",
        "",
        "print(\"✓ SQL export saved to: models/production_model.sql\")",
        "print(f\"  SQL length: {len(sql_code)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Simulate batch scoring",
        "print(\"\\nStep 3: Batch scoring simulation...\")",
        "",
        "# Create batch of new data",
        "batch_size = 1000",
        "X_batch = X_test.head(batch_size).copy()",
        "X_batch['property_id'] = range(1, batch_size + 1)",
        "",
        "# Score using saved model",
        "predictions = regressor_prod.score_from_json(",
        "    X_batch.drop('property_id', axis=1),",
        "    'models/production_model.json',",
        "    id_column=X_batch['property_id']",
        ")",
        "",
        "print(f\"✓ Scored {len(predictions)} properties\")",
        "print(\"\\nSample predictions:\")",
        "print(predictions.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Save predictions to CSV for downstream use\n",
        "print(\"\\nStep 4: Saving predictions...\")\n",
        "predictions.to_csv('models/batch_predictions.csv', index=False)\n",
        "print(\"✓ Predictions saved to: models/batch_predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Model monitoring - check prediction distribution\n",
        "print(\"\\nStep 5: Model monitoring...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Prediction distribution\n",
        "axes[0].hist(predictions['predicted'], bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "axes[0].set_xlabel('Predicted Value', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].set_title('Distribution of Predictions', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Prediction statistics\n",
        "stats_text = f\"\"\"Prediction Statistics:\n",
        "\n",
        "Mean: {predictions['predicted'].mean():.4f}\n",
        "Median: {predictions['predicted'].median():.4f}\n",
        "Std Dev: {predictions['predicted'].std():.4f}\n",
        "Min: {predictions['predicted'].min():.4f}\n",
        "Max: {predictions['predicted'].max():.4f}\n",
        "\n",
        "Total Predictions: {len(predictions)}\n",
        "\"\"\"\n",
        "\n",
        "axes[1].text(0.1, 0.5, stats_text, fontsize=12, family='monospace',\n",
        "             verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title('Summary Statistics', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Model monitoring complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 6: Comparing Sampling Methods\n",
        "\n",
        "Compare 'unique' vs 'replace' sampling strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset (smaller for quick comparison)\n",
        "diabetes = load_diabetes()\n",
        "X_diab = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "y_diab = pd.Series(diabetes.target)\n",
        "\n",
        "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
        "    X_diab, y_diab, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Diabetes dataset: {X_diab.shape[0]} samples, {X_diab.shape[1]} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with 'unique' samplingprint(\"\\nTraining with 'unique' sampling...\")regressor_unique = ParamsembleRegressor(    m=50, f=5, sample='unique', method='elastic', spread=10, random_state=42)regressor_unique.fit(X_train_d, y_train_d, X_test_d, y_test_d)y_pred_unique = regressor_unique.predict(X_test_d)r2_unique = r2_score(y_test_d, y_pred_unique)print(f\"R² with 'unique' sampling: {r2_unique:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with 'replace' samplingprint(\"\\nTraining with 'replace' sampling...\")regressor_replace = ParamsembleRegressor(    m=50, f=5, sample='replace', method='elastic', spread=10, random_state=42)regressor_replace.fit(X_train_d, y_train_d, X_test_d, y_test_d)y_pred_replace = regressor_replace.predict(X_test_d)r2_replace = r2_score(y_test_d, y_pred_replace)print(f\"R² with 'replace' sampling: {r2_replace:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare sampling methods\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "methods = ['Unique Sampling', 'Replace Sampling']\n",
        "scores = [r2_unique, r2_replace]\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "\n",
        "bars = ax.bar(methods, scores, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "ax.set_ylabel('R² Score', fontsize=12)\n",
        "ax.set_title('Sampling Method Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim([min(scores) * 0.95, max(scores) * 1.05])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, score in zip(bars, scores):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{score:.4f}',\n",
        "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary",
        "",
        "This advanced notebook demonstrated:",
        "",
        "1. **Real-world datasets** - Applied Paramsemble to California Housing and Diabetes datasets",
        "2. **Hyperparameter tuning** - Compared different configurations to optimize performance",
        "3. **Feature importance** - Analyzed which features contribute most to the ensemble",
        "4. **Model inspection** - Examined the internal structure of saved models",
        "5. **Production workflow** - Complete deployment pipeline from training to scoring",
        "6. **Sampling strategies** - Compared 'unique' vs 'replace' sampling methods",
        "",
        "### Best Practices:",
        "",
        "- **Standardize features** before training for better Lasso performance",
        "- **Tune hyperparameters** (m, f, spread) based on your dataset size and complexity",
        "- **Monitor predictions** in production to detect data drift",
        "- **Use SQL export** for high-performance database deployments",
        "- **Save models** with descriptive names and version control",
        "",
        "### Performance Tips:",
        "",
        "- Larger `m` values explore more feature combinations but increase training time",
        "- Smaller `f` values create simpler models that may generalize better",
        "- Adjust `spread` based on the number of models that outperform baseline",
        "- MARS ensembles can capture non-linear patterns but are more complex",
        "- ElasticNet ensembles are faster and often sufficient for linear relationships"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}